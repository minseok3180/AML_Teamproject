2025-06-11 04:46:00,680 - CIFAR-10 parameter setting
2025-06-11 04:46:00,680 - Epoch : 50, batch size : 16, r1 lambda : 0.005, r2 lambda : 0.005, 
2025-06-11 04:46:00,681 - ################ Training Start ################
2025-06-11 04:56:31,302 - Epoch [0], train G_Loss: 3.1846, train D_Loss: 0.6240, FID: 452.2782
2025-06-11 05:06:14,333 - Epoch [1], train G_Loss: 3.9396, train D_Loss: 0.4136, FID: 430.5741
2025-06-11 05:15:45,666 - Epoch [2], train G_Loss: 3.7802, train D_Loss: 0.3859, FID: 444.3948
2025-06-11 05:25:22,227 - Epoch [3], train G_Loss: 10.5276, train D_Loss: 0.0036, FID: 489.0268
2025-06-11 05:34:56,802 - Epoch [4], train G_Loss: 11.7495, train D_Loss: 0.0011, FID: 490.4944
2025-06-11 05:44:23,603 - Epoch [5], train G_Loss: 11.9742, train D_Loss: 0.0009, FID: 491.0793
2025-06-11 05:54:04,479 - Epoch [6], train G_Loss: 12.0202, train D_Loss: 0.0008, FID: 491.2593
2025-06-11 06:03:34,247 - Epoch [7], train G_Loss: 11.9541, train D_Loss: 0.0007, FID: 489.6368
2025-06-11 06:13:04,259 - Epoch [8], train G_Loss: 7.5006, train D_Loss: 0.3722, FID: 430.7593
2025-06-11 06:22:38,444 - Epoch [9], train G_Loss: 1.6427, train D_Loss: 0.5731, FID: 338.6838
2025-06-11 06:32:11,679 - Epoch [10], train G_Loss: 0.8759, train D_Loss: 0.6535, FID: 334.3607
2025-06-11 06:41:42,292 - Epoch [11], train G_Loss: 1.0489, train D_Loss: 0.6237, FID: 345.3360
2025-06-11 06:51:21,284 - Epoch [12], train G_Loss: 2.2011, train D_Loss: 0.5089, FID: 314.3628
2025-06-11 07:01:01,779 - Epoch [13], train G_Loss: 3.3494, train D_Loss: 0.3455, FID: 272.1284
2025-06-11 07:10:25,636 - Epoch [14], train G_Loss: 1.4228, train D_Loss: 0.5667, FID: 237.7556
2025-06-11 07:19:44,543 - Epoch [15], train G_Loss: 1.1701, train D_Loss: 0.6342, FID: 277.7658
2025-06-11 07:29:19,389 - Epoch [16], train G_Loss: 1.4650, train D_Loss: 0.5594, FID: 245.4760
2025-06-11 07:38:55,335 - Epoch [17], train G_Loss: 1.1449, train D_Loss: 0.6223, FID: 275.3496
2025-06-11 07:48:24,265 - Epoch [18], train G_Loss: 0.9513, train D_Loss: 0.6513, FID: 239.0471
2025-06-11 07:58:00,960 - Epoch [19], train G_Loss: 0.9467, train D_Loss: 0.6551, FID: 206.7237
2025-06-11 08:07:33,293 - Epoch [20], train G_Loss: 0.9471, train D_Loss: 0.6414, FID: 204.8294
2025-06-11 08:17:05,962 - Epoch [21], train G_Loss: 0.9024, train D_Loss: 0.6591, FID: 217.5070
2025-06-11 08:26:33,510 - Epoch [22], train G_Loss: 0.9166, train D_Loss: 0.6390, FID: 226.9292
2025-06-11 08:36:01,239 - Epoch [23], train G_Loss: 0.8987, train D_Loss: 0.6562, FID: 199.0416
2025-06-11 08:45:21,414 - Epoch [24], train G_Loss: 0.9134, train D_Loss: 0.6524, FID: 220.9611
2025-06-11 08:54:42,760 - Epoch [25], train G_Loss: 0.9345, train D_Loss: 0.6542, FID: 214.8790
2025-06-11 09:04:13,506 - Epoch [26], train G_Loss: 0.9303, train D_Loss: 0.6509, FID: 219.8361
2025-06-11 09:13:45,138 - Epoch [27], train G_Loss: 0.9447, train D_Loss: 0.6547, FID: 229.4563
2025-06-11 09:23:14,988 - Epoch [28], train G_Loss: 0.9462, train D_Loss: 0.6564, FID: 195.6488
2025-06-11 09:32:47,761 - Epoch [29], train G_Loss: 0.9684, train D_Loss: 0.6470, FID: 231.3402
2025-06-11 09:42:18,337 - Epoch [30], train G_Loss: 0.9322, train D_Loss: 0.6540, FID: 212.6341
2025-06-11 09:51:40,674 - Epoch [31], train G_Loss: 0.8870, train D_Loss: 0.6616, FID: 203.6882
2025-06-11 10:01:05,364 - Epoch [32], train G_Loss: 0.9638, train D_Loss: 0.6540, FID: 205.6763
2025-06-11 10:10:30,959 - Epoch [33], train G_Loss: 0.9509, train D_Loss: 0.6481, FID: 208.0851
2025-06-11 10:19:56,923 - Epoch [34], train G_Loss: 1.0615, train D_Loss: 0.6247, FID: 270.3825
2025-06-11 10:29:21,028 - Epoch [35], train G_Loss: 1.1909, train D_Loss: 0.6142, FID: 234.2590
2025-06-11 10:38:48,233 - Epoch [36], train G_Loss: 1.1675, train D_Loss: 0.6271, FID: 215.3514
2025-06-11 10:48:19,908 - Epoch [37], train G_Loss: 0.9879, train D_Loss: 0.6511, FID: 224.2595
2025-06-11 10:57:55,496 - Epoch [38], train G_Loss: 1.1643, train D_Loss: 0.6126, FID: 198.5902
2025-06-11 11:07:46,123 - Epoch [39], train G_Loss: 1.0462, train D_Loss: 0.6376, FID: 192.0709
2025-06-11 11:17:12,791 - Epoch [40], train G_Loss: 1.1484, train D_Loss: 0.6195, FID: 190.5956
2025-06-11 11:26:40,659 - Epoch [41], train G_Loss: 1.2181, train D_Loss: 0.6068, FID: 202.9115
2025-06-11 11:36:07,235 - Epoch [42], train G_Loss: 1.2187, train D_Loss: 0.6080, FID: 198.7003
2025-06-11 11:45:33,417 - Epoch [43], train G_Loss: 1.2009, train D_Loss: 0.6060, FID: 176.4190
2025-06-11 11:55:00,877 - Epoch [44], train G_Loss: 1.4135, train D_Loss: 0.5616, FID: 159.7742
2025-06-11 12:04:28,859 - Epoch [45], train G_Loss: 1.5219, train D_Loss: 0.5506, FID: 181.5861
2025-06-11 12:13:52,669 - Epoch [46], train G_Loss: 1.3917, train D_Loss: 0.5721, FID: 188.5399
2025-06-11 12:23:14,803 - Epoch [47], train G_Loss: 1.5420, train D_Loss: 0.5293, FID: 179.6339
2025-06-11 12:32:47,158 - Epoch [48], train G_Loss: 1.7453, train D_Loss: 0.5171, FID: 165.3752
2025-06-11 12:42:16,279 - Epoch [49], train G_Loss: 1.7664, train D_Loss: 0.4994, FID: 167.3507
2025-06-11 12:42:16,483 - Training finished after 50 epochs.
2025-06-11 12:42:16,483 - Final train G_Loss: 1.7664, D_Loss: 0.4994, Final FID: 167.3507
