2025-06-18 12:44:21,952 - Stacked MNIST parameter setting
2025-06-18 12:44:21,952 - Epoch : 25, batch size : 128, r1 lambda : 5, r2 lambda : 5, 
2025-06-18 12:44:21,952 - ################ Training Start ################
2025-06-18 12:44:21,952 - Stacked MNIST parameter setting
2025-06-18 12:44:21,952 - Stacked MNIST parameter setting
2025-06-18 12:44:21,952 - Epoch : 25, batch size : 128, r1 lambda : 5, r2 lambda : 5, 
2025-06-18 12:44:21,952 - Epoch : 25, batch size : 128, r1 lambda : 5, r2 lambda : 5, 
2025-06-18 12:44:21,952 - ################ Training Start ################
2025-06-18 12:44:21,952 - ################ Training Start ################
2025-06-18 12:44:21,959 - Stacked MNIST parameter setting
2025-06-18 12:44:21,959 - Epoch : 25, batch size : 128, r1 lambda : 5, r2 lambda : 5, 
2025-06-18 12:44:21,959 - ################ Training Start ################
2025-06-18 12:52:17,025 - Epoch [0], train G_Loss: 2.2210, train D_Loss: 2.1621, FID: 452.7983
2025-06-18 13:00:08,795 - Epoch [1], train G_Loss: 3.2842, train D_Loss: 0.1408, FID: 450.8406
2025-06-18 13:07:55,413 - Epoch [2], train G_Loss: 3.5929, train D_Loss: 0.1429, FID: 441.7609
2025-06-18 13:15:41,207 - Epoch [3], train G_Loss: 4.9853, train D_Loss: 0.1394, FID: 396.6196
