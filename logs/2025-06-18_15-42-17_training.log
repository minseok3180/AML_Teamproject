2025-06-18 15:42:17,795 - Stacked MNIST parameter setting
2025-06-18 15:42:17,795 - Stacked MNIST parameter setting
2025-06-18 15:42:17,795 - Stacked MNIST parameter setting
2025-06-18 15:42:17,795 - Epoch : 60, batch size : 128, r1 lambda : 1, r2 lambda : 1, 
2025-06-18 15:42:17,795 - Epoch : 60, batch size : 128, r1 lambda : 1, r2 lambda : 1, 
2025-06-18 15:42:17,795 - Epoch : 60, batch size : 128, r1 lambda : 1, r2 lambda : 1, 
2025-06-18 15:42:17,795 - ################ Training Start ################
2025-06-18 15:42:17,795 - Stacked MNIST parameter setting
2025-06-18 15:42:17,795 - ################ Training Start ################
2025-06-18 15:42:17,795 - ################ Training Start ################
2025-06-18 15:42:17,795 - Epoch : 60, batch size : 128, r1 lambda : 1, r2 lambda : 1, 
2025-06-18 15:42:17,795 - ################ Training Start ################
2025-06-18 15:50:34,385 - Epoch [0], train G_Loss: 3.6506, train D_Loss: 0.3395, FID: 468.2364
2025-06-18 15:58:48,552 - Epoch [1], train G_Loss: 5.0231, train D_Loss: 0.0293, FID: 374.5288
2025-06-18 16:06:57,694 - Epoch [2], train G_Loss: 5.3670, train D_Loss: 0.0618, FID: 471.8358
2025-06-18 16:15:07,018 - Epoch [3], train G_Loss: 5.3192, train D_Loss: 0.0398, FID: 471.8358
2025-06-18 16:23:16,995 - Epoch [4], train G_Loss: 5.5829, train D_Loss: 0.0602, FID: 471.8358
