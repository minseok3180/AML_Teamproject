2025-06-18 13:17:00,744 - Stacked MNIST parameter setting
2025-06-18 13:17:00,744 - Epoch : 60, batch size : 128, r1 lambda : 5, r2 lambda : 5, 
2025-06-18 13:17:00,744 - ################ Training Start ################
2025-06-18 13:17:00,744 - Stacked MNIST parameter setting
2025-06-18 13:17:00,745 - Epoch : 60, batch size : 128, r1 lambda : 5, r2 lambda : 5, 
2025-06-18 13:17:00,745 - ################ Training Start ################
2025-06-18 13:17:00,745 - Stacked MNIST parameter setting
2025-06-18 13:17:00,748 - Epoch : 60, batch size : 128, r1 lambda : 5, r2 lambda : 5, 
2025-06-18 13:17:00,748 - ################ Training Start ################
2025-06-18 13:17:00,751 - Stacked MNIST parameter setting
2025-06-18 13:17:00,751 - Epoch : 60, batch size : 128, r1 lambda : 5, r2 lambda : 5, 
2025-06-18 13:17:00,751 - ################ Training Start ################
2025-06-18 13:24:57,152 - Epoch [0], train G_Loss: 2.6317, train D_Loss: 1.2742, FID: 440.1041
2025-06-18 13:32:52,178 - Epoch [1], train G_Loss: 3.2901, train D_Loss: 0.1251, FID: 403.8290
2025-06-18 13:40:38,523 - Epoch [2], train G_Loss: 4.6409, train D_Loss: 0.2465, FID: 353.2658
2025-06-18 13:48:24,464 - Epoch [3], train G_Loss: 5.7786, train D_Loss: 0.0973, FID: 353.2658
2025-06-18 13:56:10,482 - Epoch [4], train G_Loss: 6.1820, train D_Loss: 0.0680, FID: 353.2658
2025-06-18 14:03:56,212 - Epoch [5], train G_Loss: 6.1911, train D_Loss: 0.0524, FID: 353.2658
2025-06-18 14:11:41,561 - Epoch [6], train G_Loss: 5.9907, train D_Loss: 0.0530, FID: 353.2658
2025-06-18 14:19:27,106 - Epoch [7], train G_Loss: 6.5092, train D_Loss: 0.0106, FID: 353.2658
2025-06-18 14:27:12,069 - Epoch [8], train G_Loss: 6.4585, train D_Loss: 0.0212, FID: 353.2658
2025-06-18 14:34:58,012 - Epoch [9], train G_Loss: 5.6311, train D_Loss: 0.4142, FID: 353.2658
2025-06-18 14:42:43,653 - Epoch [10], train G_Loss: 5.3185, train D_Loss: 0.0328, FID: 353.2658
